{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "lm_B반_2조_양명철.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYsJ-N2cEX-e",
        "colab_type": "text"
      },
      "source": [
        "**자연어 처리 과제**\n",
        "\n",
        "B반 2조 양명철"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apeIx-s36p04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.utils.rnn as rnn\n",
        "import statistics\n",
        "import nltk\n",
        "import random\n",
        "import collections\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfEsLTXK6p1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Set cuda if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAhdoph6p1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dictionary class 선언\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self, dataset, size):\n",
        "        ## init vocab ##\n",
        "        self.word2idx = {'<pad>':0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.idx2word = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "\n",
        "        self.build_dict(dataset, size)\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, 3) # if word does not exist in vocab then return unk idx\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def build_dict(self, dataset, dict_size):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        total_words = (word for sent in dataset for word in sent)\n",
        "        word_freq = collections.Counter(total_words) # count the number of each word\n",
        "        vocab = sorted(word_freq.keys(), key=lambda word: (-word_freq[word], word)) # sort by frequency\n",
        "        vocab = vocab[:dict_size]\n",
        "        for word in vocab:\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4vsSjGP6p1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Brown dataset Preprocessing (NLTK)\n",
        "def brown_dataset(min=5, max=30):\n",
        "    nltk.download('brown')\n",
        "\n",
        "    # get sentences with the length between min and max\n",
        "    # convert all words into lower-case\n",
        "    all_seq = [[token.lower() for token in seq] for seq in nltk.corpus.brown.sents() if min <= len(seq) <= max]\n",
        "\n",
        "    random.shuffle(all_seq) # shuffle\n",
        "    return all_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPCi60Ns6p14",
        "colab_type": "code",
        "outputId": "b44ea97d-ebe1-4bbf-a276-e586257627e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "## Download Brown dataset\n",
        "dataset = brown_dataset()\n",
        "\n",
        "## print some part\n",
        "print(dataset[0])\n",
        "print(dataset[1])\n",
        "print(dataset[2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "['the', 'patrol', 'snaked', 'around', 'in', 'back', 'of', 'the', 'cave', ',', 'approached', 'it', 'from', 'above', 'and', 'dropped', 'in', 'suddenly', 'with', 'wild', 'howls', '.']\n",
            "['powers', 'knocked', 'his', 'arm', 'aside', '.']\n",
            "['today', ',', 'federal', 'funds', 'account', 'for', 'only', 'one-fifth', 'of', 'the', \"nation's\", 'expenditures', 'for', 'vocational', 'education', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evTxpKel6p2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data Handling class 선언\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, dataset, device, dict_size=20000, train_ratio=0.95):\n",
        "        train_size = int(len(dataset) * train_ratio)\n",
        "        valid_size = len(dataset) - train_size\n",
        "        self.device = device\n",
        "        self.dictionary = Dictionary(dataset, dict_size)\n",
        "        self.train = dataset[:train_size]\n",
        "        self.valid = dataset[:valid_size]\n",
        "\n",
        "    def indexing(self, dat):\n",
        "        src_idxes = []\n",
        "        tgt_idxes = []\n",
        "        for sent in dat:\n",
        "            src_idx = [self.dictionary('<sos>')] + [self.dictionary(word) for word in sent]\n",
        "            tgt_idx = [self.dictionary(word) for word in sent] + [self.dictionary('<eos>')]\n",
        "            src_idxes.append(torch.tensor(src_idx).type(torch.int64))\n",
        "            tgt_idxes.append(torch.tensor(tgt_idx).type(torch.int64))\n",
        "\n",
        "        src_idxes = rnn.pad_sequence(src_idxes, batch_first=True).to(self.device) # shape = [B, L]\n",
        "        tgt_idxes = rnn.pad_sequence(tgt_idxes, batch_first=True).to(self.device).view(-1) # flatten shape = [B * L]\n",
        "\n",
        "        return src_idxes, tgt_idxes\n",
        "\n",
        "    def batch_iter(self, batch_size, isTrain=True):\n",
        "        dat = self.train if isTrain else self.valid\n",
        "        if isTrain:\n",
        "            random.shuffle(dat)\n",
        "\n",
        "        for i in range(len(dat) // batch_size):\n",
        "            batch = dat[i * batch_size: (i+1) * batch_size]\n",
        "            src, tgt = self.indexing(batch)\n",
        "            yield {'src': src, 'tgt': tgt}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_b7DAqo6p2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(dataset, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhXz4OQN6p2l",
        "colab_type": "code",
        "outputId": "287bb536-1b5d-4d79-c945-3da4695e383d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Dictionary 확인\n",
        "\n",
        "for i, (key, val) in enumerate(corpus.dictionary.word2idx.items()):\n",
        "    print('word:  {:10s} | index: {:5d} '.format(key, val))\n",
        "    if i == 20:\n",
        "        break\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word:  <pad>      | index:     0 \n",
            "word:  <sos>      | index:     1 \n",
            "word:  <eos>      | index:     2 \n",
            "word:  <unk>      | index:     3 \n",
            "word:  the        | index:     4 \n",
            "word:  .          | index:     5 \n",
            "word:  ,          | index:     6 \n",
            "word:  of         | index:     7 \n",
            "word:  and        | index:     8 \n",
            "word:  to         | index:     9 \n",
            "word:  a          | index:    10 \n",
            "word:  in         | index:    11 \n",
            "word:  was        | index:    12 \n",
            "word:  he         | index:    13 \n",
            "word:  is         | index:    14 \n",
            "word:  ''         | index:    15 \n",
            "word:  ``         | index:    16 \n",
            "word:  it         | index:    17 \n",
            "word:  that       | index:    18 \n",
            "word:  for        | index:    19 \n",
            "word:  ;          | index:    20 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3raD85M6p21",
        "colab_type": "code",
        "outputId": "c0d0ad59-04cb-40f9-b3a4-125012c64202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "## indexing 함수 결과 확인\n",
        "\n",
        "# case : 단일 문장 입력 시. \n",
        "sent = [dataset[1]]\n",
        "idx_src, idx_tgt = corpus.indexing(sent)\n",
        "\n",
        "print(sent)\n",
        "print(idx_src) # <SOS> index로 시작\n",
        "print(idx_tgt) # <EOS> index로 종료\n",
        "\n",
        "print('-' * 90)\n",
        "## case : 복수 문장 입력 시 (batching)\n",
        "batch = [dataset[0], dataset[1]]\n",
        "idx_src, idx_tgt = corpus.indexing(batch)\n",
        "\n",
        "print(batch)\n",
        "print(idx_src) # 가장 길이가 긴 문장 (dataset[0]) 보다 짧은 문장 (dataset[1]) 의 경우 남는 길이만큼 padding=0 삽입 확인.\n",
        "print(idx_tgt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['powers', 'knocked', 'his', 'arm', 'aside', '.']]\n",
            "tensor([[   1, 1636, 2797,   21,  986, 1656,    5]], device='cuda:0')\n",
            "tensor([1636, 2797,   21,  986, 1656,    5,    2], device='cuda:0')\n",
            "------------------------------------------------------------------------------------------\n",
            "[['the', 'patrol', 'snaked', 'around', 'in', 'back', 'of', 'the', 'cave', ',', 'approached', 'it', 'from', 'above', 'and', 'dropped', 'in', 'suddenly', 'with', 'wild', 'howls', '.'], ['powers', 'knocked', 'his', 'arm', 'aside', '.']]\n",
            "tensor([[   1,    4, 3616,    3,  155,   11,  102,    7,    4, 6584,    6, 1918,\n",
            "           17,   36,  299,    8,  940,   11,  645,   22, 2023,    3,    5],\n",
            "        [   1, 1636, 2797,   21,  986, 1656,    5,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
            "       device='cuda:0')\n",
            "tensor([   4, 3616,    3,  155,   11,  102,    7,    4, 6584,    6, 1918,   17,\n",
            "          36,  299,    8,  940,   11,  645,   22, 2023,    3,    5,    2, 1636,\n",
            "        2797,   21,  986, 1656,    5,    2,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmiMDxwC6p3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## RNN Language model 선언\n",
        "\n",
        "# Define network\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, ntoken, hidden_size, nlayers, dropout=0.1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.embeddings = nn.Embedding(ntoken, hidden_size, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, nlayers, dropout=dropout, batch_first=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, ntoken)\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.ntoken = ntoken\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_layer.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.embeddings(input) # emb = (batch, length, dim)\n",
        "        output, hidden = self.rnn(emb, hidden) # output = (batch. length. dim)\n",
        "        output = self.drop(output)\n",
        "        output = self.output_layer(output)\n",
        "        output = output.view(-1, self.ntoken) # output = (batch * length, dim)\n",
        "\n",
        "        return self.sm(output), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()) # to set init tensor with the same torch.dtype and torch.device\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.hidden_size),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.hidden_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBVHtlc66p3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 40\n",
        "hidden_size = 256\n",
        "dropout = 0.2\n",
        "max_epoch = 30\n",
        "\n",
        "# build model\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel(ntokens, hidden_size, 2, dropout).to(device)\n",
        "\n",
        "# set loss func and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.NLLLoss(ignore_index=0, reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG5VCrUf6p3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Training / Evaluation Parts #######"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwwEeIXP6p3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy\n",
        "def cal_acc(scores, target):\n",
        "    pred = scores.max(-1)[1]\n",
        "    non_pad = target.ne(0)\n",
        "    num_correct = pred.eq(target).masked_select(non_pad).sum().item() \n",
        "    num_non_pad = non_pad.sum().item()\n",
        "    return 100 * (num_correct / num_non_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lQGcqq56p3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train func.\n",
        "def train():\n",
        "    model.train() # Turn on training mode which enables dropout.\n",
        "    mean_loss = []\n",
        "    mean_acc = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in corpus.batch_iter(batch_size):\n",
        "        hidden = model.init_hidden(batch_size) # zero vectors for init hidden\n",
        "        target = batch['tgt'] # flattened target \n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(batch['src'], hidden) # output = flatten output = [Batch_size * Length, vocab_size]\n",
        "\n",
        "        loss = criterion(output, target) # compare between vocab_prob and answer_prob(one-hot converted)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_loss.append(loss.item())\n",
        "        mean_acc.append(cal_acc(output, target))\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    mean_acc = statistics.mean(mean_acc)\n",
        "    mean_loss = statistics.mean(mean_loss)\n",
        "\n",
        "    return mean_loss, total_time, mean_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDUYAz66p37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluation func.\n",
        "def evaluate():\n",
        "    model.eval() # Turn off dropout\n",
        "    mean_loss = []\n",
        "    mean_acc = []\n",
        "\n",
        "    for batch in corpus.batch_iter(batch_size, isTrain=False):\n",
        "        with torch.no_grad():\n",
        "            hidden = model.init_hidden(batch_size)\n",
        "            target = batch['tgt']\n",
        "            output, hidden = model(batch['src'], hidden)\n",
        "            loss = criterion(output, target)\n",
        "            mean_loss.append(loss.item())\n",
        "            mean_acc.append(cal_acc(output, target))\n",
        "\n",
        "    mean_acc = statistics.mean(mean_acc)\n",
        "    mean_loss = statistics.mean(mean_loss)\n",
        "\n",
        "    return mean_loss, mean_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee6TMVms6p4O",
        "colab_type": "code",
        "outputId": "510dbad8-4477-4f05-a9fe-aa3886d7d367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "isTrain=True # Flag variable\n",
        "\n",
        "if isTrain: # set False if you don't need to train model\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, max_epoch+1):\n",
        "        loss, epoch_time, accuracy = train()\n",
        "        print('epoch {:4d} | times {:3.3f} |  loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch+1, epoch_time, loss, accuracy))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss, accuracy = evaluate()\n",
        "            print('=' * 60)\n",
        "            print('Evaluation | loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch+1, epoch_time, loss, accuracy))\n",
        "            print('=' * 60)\n",
        "\n",
        "    with open('model.pt', 'wb') as f:\n",
        "        print('save model at: ./model.pt')\n",
        "        torch.save(model, f)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch    2 | times 26.742 |  loss: 6.031 | accuracy: 17.37\n",
            "epoch    3 | times 27.406 |  loss: 5.419 | accuracy: 21.66\n",
            "epoch    4 | times 27.107 |  loss: 5.176 | accuracy: 22.89\n",
            "epoch    5 | times 27.074 |  loss: 4.989 | accuracy: 23.71\n",
            "epoch    6 | times 27.246 |  loss: 4.832 | accuracy: 24.32\n",
            "epoch    7 | times 27.217 |  loss: 4.693 | accuracy: 24.84\n",
            "epoch    8 | times 27.174 |  loss: 4.572 | accuracy: 25.26\n",
            "epoch    9 | times 27.319 |  loss: 4.461 | accuracy: 25.64\n",
            "epoch   10 | times 27.289 |  loss: 4.357 | accuracy: 26.01\n",
            "epoch   11 | times 27.069 |  loss: 4.266 | accuracy: 26.38\n",
            "============================================================\n",
            "Evaluation | loss: 11.000 | accuracy: 27.07\n",
            "============================================================\n",
            "epoch   12 | times 27.096 |  loss: 4.182 | accuracy: 26.72\n",
            "epoch   13 | times 27.148 |  loss: 4.108 | accuracy: 27.11\n",
            "epoch   14 | times 27.236 |  loss: 4.045 | accuracy: 27.44\n",
            "epoch   15 | times 27.159 |  loss: 3.985 | accuracy: 27.78\n",
            "epoch   16 | times 27.189 |  loss: 3.937 | accuracy: 28.09\n",
            "epoch   17 | times 27.267 |  loss: 3.891 | accuracy: 28.44\n",
            "epoch   18 | times 27.277 |  loss: 3.851 | accuracy: 28.81\n",
            "epoch   19 | times 27.198 |  loss: 3.813 | accuracy: 29.05\n",
            "epoch   20 | times 27.244 |  loss: 3.783 | accuracy: 29.24\n",
            "epoch   21 | times 27.139 |  loss: 3.752 | accuracy: 29.54\n",
            "============================================================\n",
            "Evaluation | loss: 21.000 | accuracy: 27.14\n",
            "============================================================\n",
            "epoch   22 | times 27.184 |  loss: 3.727 | accuracy: 29.75\n",
            "epoch   23 | times 27.291 |  loss: 3.703 | accuracy: 29.95\n",
            "epoch   24 | times 27.167 |  loss: 3.679 | accuracy: 30.17\n",
            "epoch   25 | times 27.072 |  loss: 3.657 | accuracy: 30.39\n",
            "epoch   26 | times 27.020 |  loss: 3.640 | accuracy: 30.57\n",
            "epoch   27 | times 27.062 |  loss: 3.622 | accuracy: 30.71\n",
            "epoch   28 | times 27.017 |  loss: 3.607 | accuracy: 30.90\n",
            "epoch   29 | times 27.036 |  loss: 3.592 | accuracy: 31.01\n",
            "epoch   30 | times 27.060 |  loss: 3.579 | accuracy: 31.13\n",
            "epoch   31 | times 27.022 |  loss: 3.563 | accuracy: 31.28\n",
            "============================================================\n",
            "Evaluation | loss: 31.000 | accuracy: 27.02\n",
            "============================================================\n",
            "save model at: ./model.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9nrgHiHgWF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = RNNModel(ntokens, hidden_size, 2, dropout).to(device)\n",
        "def pred_sent_prob(sent):\n",
        "    import numpy as np\n",
        "    model.eval()\n",
        "    sent_prob = 0\n",
        "     # 1. construct input for the model -> corpus, indexing 이용\n",
        "    idx_src, idx_tgt = corpus.indexing(sent)\n",
        "    with torch.no_grad():\n",
        "      # 2. set init hidden: 첫번째 step에 대한 zero vector\n",
        "      hidden = model.init_hidden(len(sent))\n",
        "      # 3. get model output --> log softmax 값이 output으로 출력됨. (즉, 전체 단어에 대한 log 확률값이 출력됨.)\n",
        "      output, hidden = model(idx_src, hidden)\n",
        "      for i in range(len(idx_tgt)):\n",
        "        # 4. get word log_prob --> output 각각에 대해 입력 단어의 확률을 찾는다\n",
        "        sent_prob += output[i][idx_tgt[i]] # 5. comput sentence log_prob by summing each of word log_prob --> 각각 단어 확률을 모두 더한다.\n",
        "\n",
        "    return sent_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeiE5t4o6p4o",
        "colab_type": "code",
        "outputId": "069bac8c-7f52-4515-82ba-836e9b7c5081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# load saved model\n",
        "with open('./model.pt', 'rb') as f:\n",
        "    print('load model from: ./model.pt')\n",
        "    model = torch.load(f).to(device)\n",
        "\n",
        "    print('log prob of [the dog bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'dog', 'bark', '.']])))\n",
        "    print('log prob of [the cat bark .]: {:3.3f}'.format(pred_sent_prob([['the', 'cat', 'bark', '.']])))\n",
        "\n",
        "    print('log prob of [boy am a i .]: {:3.3f}'.format(pred_sent_prob([['boy', 'am', 'a', 'i', '.']])))\n",
        "    print('log prob of [i am a boy .]: {:3.3f}'.format(pred_sent_prob([['i', 'am', 'a', 'boy', '.']])))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load model from: ./model.pt\n",
            "log prob of [the dog bark .]: -41.560\n",
            "log prob of [the cat bark .]: -57.360\n",
            "log prob of [boy am a i .]: -38.478\n",
            "log prob of [i am a boy .]: -20.281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKao0-kZ6p4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model from: ./model.pt\n",
        "# log prob of [the dog bark .]: -34.680\n",
        "# log prob of [the cat bark .]: -52.044\n",
        "# log prob of [boy am a i .]: -50.034\n",
        "# log prob of [i am a boy .]: -20.275"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}